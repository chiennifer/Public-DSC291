{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local files\n",
    "\n",
    "### Download and Uplaod\n",
    "\n",
    "Download and upload files via the Spark Notebook interface.\n",
    "\n",
    "### Access Local Files\n",
    "\n",
    "The file path to local files requires `file://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/passwd\r\n"
     ]
    }
   ],
   "source": [
    "ls /etc/passwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'root:x:0:0:root:/root:/bin/bash',\n",
       " u'bin:x:1:1:bin:/bin:/sbin/nologin',\n",
       " u'daemon:x:2:2:daemon:/sbin:/sbin/nologin',\n",
       " u'adm:x:3:4:adm:/var/adm:/sbin/nologin',\n",
       " u'lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin',\n",
       " u'sync:x:5:0:sync:/sbin:/bin/sync',\n",
       " u'shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown',\n",
       " u'halt:x:7:0:halt:/sbin:/sbin/halt',\n",
       " u'mail:x:8:12:mail:/var/spool/mail:/sbin/nologin',\n",
       " u'uucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin',\n",
       " u'operator:x:11:0:operator:/root:/sbin/nologin',\n",
       " u'games:x:12:100:games:/usr/games:/sbin/nologin',\n",
       " u'gopher:x:13:30:gopher:/var/gopher:/sbin/nologin',\n",
       " u'ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin',\n",
       " u'nobody:x:99:99:Nobody:/:/sbin/nologin',\n",
       " u'rpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin',\n",
       " u'ntp:x:38:38::/etc/ntp:/sbin/nologin',\n",
       " u'saslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin',\n",
       " u'mailnull:x:47:47::/var/spool/mqueue:/sbin/nologin',\n",
       " u'smmsp:x:51:51::/var/spool/mqueue:/sbin/nologin',\n",
       " u'rpcuser:x:29:29:RPC Service User:/var/lib/nfs:/sbin/nologin',\n",
       " u'nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin',\n",
       " u'sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin',\n",
       " u'dbus:x:81:81:System message bus:/:/sbin/nologin',\n",
       " u'hadoop:x:498:500::/home/hadoop:/bin/bash',\n",
       " u'puppet:x:497:497:puppet:/var/lib/puppet:/sbin/nologin',\n",
       " u'ec2-user:x:500:501:EC2 Default User:/home/ec2-user:/bin/bash',\n",
       " u'zookeeper:x:496:496:ZooKeeper:/var/lib/zookeeper:/sbin/nologin',\n",
       " u'yarn:x:495:495:Hadoop Yarn:/var/lib/hadoop-yarn:/bin/bash',\n",
       " u'hdfs:x:494:494:Hadoop HDFS:/var/lib/hadoop-hdfs:/bin/bash',\n",
       " u'mapred:x:493:493:Hadoop MapReduce:/var/lib/hadoop-mapreduce:/bin/bash']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_files = sc.textFile(\"file:///etc/passwd\")\n",
    "local_files.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s3helper\n",
    "\n",
    "The object `s3helper` is a tool to transfer files between local filesystem, HDFS and S3.\n",
    "\n",
    "Run `s3helper.help()` to learn all its methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        s3helper is a helper object to move files and directory between\n",
      "        local filesystem, AWS S3 and local HDFS.\n",
      "\n",
      "        Usage:\n",
      "\n",
      "        1. Open a S3 bucket under your account\n",
      "            s3helper.open_bucket(<bucket_name>)\n",
      "        2. List all files under the opened S3 bucket\n",
      "            s3helper.ls() or s3helper.ls_s3()\n",
      "        Or optionally,\n",
      "            s3helper.ls(<file_path>) or s3helper.ls_s3(<file_path>)\n",
      "        3. List all files on HDFS\n",
      "            s3helper.ls_hdfs()\n",
      "        Or optionally,\n",
      "            s3helper.ls_hdfs(<file_path>)\n",
      "        where <file_path> is an absolute path in the opened S3 bucket.\n",
      "\n",
      "        Now you can access your S3 files.\n",
      "\n",
      "        1. Transfer files between S3 and HDFS\n",
      "          a. To download all S3 files under a directory to HDFS, please call\n",
      "                s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)\n",
      "          b. To upload a directory on HDFS to S3, please call\n",
      "                s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)\n",
      "\n",
      "        2. Transfer files between S3 and local filesystem (not HDFS)\n",
      "          a. To download one single S3 file to local filesystem, please call\n",
      "                s3helper.s3_to_local(<s3_file_path>, <local_file_path>)\n",
      "          b. To upload a file on local filesystem to S3, please call\n",
      "                s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)\n",
      "\n",
      "        3. Transfer files between local filesystem and HDFS\n",
      "          a. To upload a directory on local filesystem to HDFS, please call\n",
      "                s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)\n",
      "\n",
      "        4. Get S3 file paths without data transfer\n",
      "          a. To get the URLs of S3 files under a directory, please call\n",
      "                s3helper.get_path(<s3_directory_path>)\n",
      "          Note this method do nothing on your local HDFS.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "s3helper.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Open the bucket that has your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket('dse-team')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) List files in the S3 bucket and HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fromHDFS']\n",
      "[u'fromHDFS/', u'fromHDFS/README']\n",
      "Found 4 items\n",
      "drwxr-xr-x   - hdfs hadoop          0 2017-05-25 16:02 /apps\n",
      "drwxrwxrwt   - hdfs hadoop          0 2017-05-25 16:07 /tmp\n",
      "drwxr-xr-x   - hdfs hadoop          0 2017-05-25 16:02 /user\n",
      "drwxr-xr-x   - hdfs hadoop          0 2017-05-25 16:02 /var\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_s3())  # By default, list all files in the root directory of the bucket\n",
    "print(s3helper.ls_s3('fromHDFS'))\n",
    "\n",
    "print(s3helper.ls_hdfs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Move files around local filesystem, HDFS and S3\n",
    "\n",
    "As described in `s3helper.help()`, there are five methods for file transfers:\n",
    "\n",
    "1. `s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)`\n",
    "2. `s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)`\n",
    "3. `s3helper.s3_to_local(<s3_file_path>, <local_file_path>)`\n",
    "4. `s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)`\n",
    "5. `s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.local_to_s3(\"/home/hadoop/data/data.dat\", \"fromLocal/data.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fromLocal/data.dat']\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_s3(\"fromLocal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/05/25 18:03:58 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-team/fromLocal/data.dat' for reading\n",
      "\n",
      "Found 1 items\n",
      "-rw-r--r--   1 hadoop hadoop  484472684 2017-05-25 18:04 /test/fromS3/data.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3helper.s3_to_hdfs(\"fromLocal/data.dat\", \"/test/fromS3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 hadoop hadoop  484472684 2017-05-25 18:04 /test/fromS3/data.dat\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_hdfs(\"/test/fromS3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir /home/hadoop/fromS3\n",
    "mkdir /home/hadoop/fromHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.s3_to_local(\"fromLocal/data.dat\", \"/home/hadoop/fromS3/data.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.dat\r\n"
     ]
    }
   ],
   "source": [
    "ls ~/fromS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.local_to_hdfs(\"/home/hadoop/fromS3/\", \"/test/fromLocal/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 hadoop hadoop  484472684 2017-05-25 18:08 /test/fromLocal/data.dat\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_hdfs(\"/test/fromLocal/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*NOTE*\n",
      "This method will create a MapReudce job to upload the content in HDFS to S3. The process may take a while.\n",
      "\n",
      "\n",
      "17/05/25 18:09:02 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsize', sourceFileListing=null, sourcePaths=[/test/fromLocal], targetPath=s3n://dse-team/fromHDFS, targetPathExists=true, preserveRawXattrs=false}\n",
      "17/05/25 18:09:03 INFO impl.TimelineClientImpl: Timeline service address: http://ip-172-31-70-194.ec2.internal:8188/ws/v1/timeline/\n",
      "17/05/25 18:09:03 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-70-194.ec2.internal/172.31.70.194:8032\n",
      "17/05/25 18:09:04 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "17/05/25 18:09:04 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "17/05/25 18:09:04 INFO impl.TimelineClientImpl: Timeline service address: http://ip-172-31-70-194.ec2.internal:8188/ws/v1/timeline/\n",
      "17/05/25 18:09:04 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-70-194.ec2.internal/172.31.70.194:8032\n",
      "17/05/25 18:09:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/25 18:09:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495728137505_0008\n",
      "17/05/25 18:09:05 INFO impl.YarnClientImpl: Submitted application application_1495728137505_0008\n",
      "17/05/25 18:09:05 INFO mapreduce.Job: The url to track the job: http://ip-172-31-70-194.ec2.internal:20888/proxy/application_1495728137505_0008/\n",
      "17/05/25 18:09:05 INFO tools.DistCp: DistCp job-id: job_1495728137505_0008\n",
      "17/05/25 18:09:05 INFO mapreduce.Job: Running job: job_1495728137505_0008\n",
      "17/05/25 18:09:13 INFO mapreduce.Job: Job job_1495728137505_0008 running in uber mode : false\n",
      "17/05/25 18:09:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/25 18:09:22 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/25 18:09:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/25 18:09:40 INFO mapreduce.Job: Job job_1495728137505_0008 completed successfully\n",
      "17/05/25 18:09:40 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=259600\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=484473665\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=17\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tS3N: Number of bytes read=0\n",
      "\t\tS3N: Number of bytes written=484472684\n",
      "\t\tS3N: Number of read operations=0\n",
      "\t\tS3N: Number of large read operations=0\n",
      "\t\tS3N: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=883200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=27600\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=27600\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=28262400\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=274\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=445\n",
      "\t\tCPU time spent (ms)=24240\n",
      "\t\tPhysical memory (bytes) snapshot=808022016\n",
      "\t\tVirtual memory (bytes) snapshot=6595624960\n",
      "\t\tTotal committed heap usage (bytes)=737673216\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "\torg.apache.hadoop.tools.mapred.CopyMapper$Counter\n",
      "\t\tBYTESCOPIED=484472684\n",
      "\t\tBYTESEXPECTED=484472684\n",
      "\t\tCOPY=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3helper.hdfs_to_s3(\"/test/fromLocal\", \"fromHDFS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fromHDFS/fromLocal/data.dat', u'fromHDFS/fromLocal_$folder$']\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_s3(\"fromHDFS/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Files\n",
    "\n",
    "To get a reasonable reading speed, please always load parquet files from S3 to HDFS before accessing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket(\"your-bucket-name\")\n",
    "\n",
    "files = s3helper.s3_to_hdfs('/sub-directory-for-parquets', '/hdfs-directory.parquet')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.sql(\"SELECT key, value FROM parquet.`/hdfs-directory.parquet`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(master=master_url)\n",
    "sqlc = SQLContext(sc)\n",
    "rdd = sc.parallelize([\n",
    "        [\"name\", \"age\"],\n",
    "        [\"John\", 20],\n",
    "        [\"Mike\", 25],\n",
    "        [\"Mary\", 21]\n",
    "    ])\n",
    "df = sqlc.createDataFrame(rdd)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: You have loaded library /root/ephemeral-hdfs/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "2016-08-13 07:38:38,699 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 5 items\n",
      "-rw-r--r--   3 root supergroup          0 2016-08-13 07:38 /mydata/_SUCCESS\n",
      "-rw-r--r--   3 root supergroup        283 2016-08-13 07:38 /mydata/_common_metadata\n",
      "-rw-r--r--   3 root supergroup        719 2016-08-13 07:38 /mydata/_metadata\n",
      "-rw-r--r--   3 root supergroup        515 2016-08-13 07:38 /mydata/part-r-00000-b05d9e6d-39b2-4bf3-9496-40f74e43671b.gz.parquet\n",
      "-rw-r--r--   3 root supergroup        508 2016-08-13 07:38 /mydata/part-r-00001-b05d9e6d-39b2-4bf3-9496-40f74e43671b.gz.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.save(\"hdfs:///mydata\")\n",
    "s3helper.ls_hdfs(\"/mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*NOTE*\n",
      "This method will create a MapReudce job to upload the content in HDFS to S3. The process may take a while.\n",
      "\n",
      "\n",
      "2016-08-13 07:38:39,551 INFO  [main] tools.DistCp (DistCp.java:run(109)) - Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsize', sourceFileListing=null, sourcePaths=[/mydata], targetPath=s3n://ucsd-twitter/mydata}\n",
      "2016-08-13 07:38:40,579 INFO  [main] client.RMProxy (RMProxy.java:createRMProxy(92)) - Connecting to ResourceManager at ec2-54-86-169-42.compute-1.amazonaws.com/172.31.56.20:8032\n",
      "OpenJDK 64-Bit Server VM warning: You have loaded library /root/ephemeral-hdfs/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "2016-08-13 07:38:41,538 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2016-08-13 07:38:43,069 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2016-08-13 07:38:43,070 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2016-08-13 07:38:43,235 INFO  [main] client.RMProxy (RMProxy.java:createRMProxy(92)) - Connecting to ResourceManager at ec2-54-86-169-42.compute-1.amazonaws.com/172.31.56.20:8032\n",
      "2016-08-13 07:38:43,450 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(396)) - number of splits:5\n",
      "2016-08-13 07:38:43,799 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(479)) - Submitting tokens for job: job_1471069773259_0006\n",
      "2016-08-13 07:38:44,021 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(204)) - Submitted application application_1471069773259_0006\n",
      "2016-08-13 07:38:44,064 INFO  [main] mapreduce.Job (Job.java:submit(1289)) - The url to track the job: http://ec2-54-86-169-42.compute-1.amazonaws.com:8088/proxy/application_1471069773259_0006/\n",
      "2016-08-13 07:38:44,065 INFO  [main] tools.DistCp (DistCp.java:execute(164)) - DistCp job-id: job_1471069773259_0006\n",
      "2016-08-13 07:38:44,065 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1334)) - Running job: job_1471069773259_0006\n",
      "2016-08-13 07:38:51,556 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1355)) - Job job_1471069773259_0006 running in uber mode : false\n",
      "2016-08-13 07:38:51,557 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 0% reduce 0%\n",
      "2016-08-13 07:38:58,628 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 20% reduce 0%\n",
      "2016-08-13 07:39:05,676 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 40% reduce 0%\n",
      "2016-08-13 07:39:11,718 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 60% reduce 0%\n",
      "2016-08-13 07:39:17,760 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 80% reduce 0%\n",
      "2016-08-13 07:39:24,815 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1362)) -  map 100% reduce 0%\n",
      "2016-08-13 07:39:25,835 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1373)) - Job job_1471069773259_0006 completed successfully\n",
      "2016-08-13 07:39:25,971 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Counters: 40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=482220\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4141\n",
      "\t\tHDFS: Number of bytes written=234\n",
      "\t\tHDFS: Number of read operations=41\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=10\n",
      "\t\tS3N: Number of bytes read=0\n",
      "\t\tS3N: Number of bytes written=1023\n",
      "\t\tS3N: Number of read operations=0\n",
      "\t\tS3N: Number of large read operations=0\n",
      "\t\tS3N: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=5\n",
      "\t\tOther local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25168\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=25168\n",
      "\t\tTotal vcore-seconds taken by all map tasks=25168\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25772032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=3\n",
      "\t\tInput split bytes=670\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=459\n",
      "\t\tCPU time spent (ms)=7460\n",
      "\t\tPhysical memory (bytes) snapshot=597229568\n",
      "\t\tVirtual memory (bytes) snapshot=4743868416\n",
      "\t\tTotal committed heap usage (bytes)=159580160\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2448\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=234\n",
      "\torg.apache.hadoop.tools.mapred.CopyMapper$Counter\n",
      "\t\tBYTESCOPIED=1023\n",
      "\t\tBYTESEXPECTED=1023\n",
      "\t\tBYTESSKIPPED=1002\n",
      "\t\tCOPY=2\n",
      "\t\tSKIP=3\n"
     ]
    }
   ],
   "source": [
    "s3helper.open_bucket(\"<your_bucket_name>\")\n",
    "s3helper.hdfs_to_s3(\"/mydata\", \"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'mydata/_SUCCESS',\n",
       " u'mydata/_common_metadata',\n",
       " u'mydata/_metadata',\n",
       " u'mydata/mydata',\n",
       " u'mydata/mydata_$folder$',\n",
       " u'mydata/part-r-00000-8e374f2c-b5a7-45ed-ae62-4279db81a48c.gz.parquet',\n",
       " u'mydata/part-r-00001-8e374f2c-b5a7-45ed-ae62-4279db81a48c.gz.parquet']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.ls_s3(\"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To load data back to HDFS from S3 next time, you can run\n",
    "s3helper.s3_to_hdfs(\"mydata\", \"/mydata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
