{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:24:45.090816Z",
     "start_time": "2018-04-26T22:24:45.087493Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNY.parquet\u001b[m\u001b[m                STAT_NY.pickle            index.html\r\n",
      "NY.tgz                    \u001b[34mdecon_NY_PRCP_s20.parquet\u001b[m\u001b[m index.html~\r\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "data_dir='../../../Data/Weather'\n",
    "!ls $data_dir\n",
    "state='NY'\n",
    "m='SNWD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral Analysis of Snow Depth in NY state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img alt=\"\" src=\"Figures/MeanStdSNWD_NY.png\" style=\"width:800px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Loading libdaries and data\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:24:20.962179Z",
     "start_time": "2018-04-26T22:24:20.313486Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "    pandas as    pd \tversion=1.0.1 \trequired version>=0.19.2\n",
      "\n",
      "     numpy as    np \tversion=1.18.1 \trequired version>=1.12.0\n",
      "\n",
      "   sklearn as    sk \tversion=0.22.1 \trequired version>=0.18.1\n",
      "\n",
      "module urllib has no version\n",
      "   pyspark as pyspark \tversion=2.3.0 \trequired version>=2.1.0\n",
      "\n",
      "ipywidgets as ipywidgets \tversion=7.5.1 \trequired version>=6.0.0\n",
      "\n",
      "version of ipwidgets= 7.5.1\n"
     ]
    }
   ],
   "source": [
    "# Enable automiatic reload of libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # means that all modules are reloaded before every command\n",
    "\n",
    "%pylab inline\n",
    "#%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('./lib')\n",
    "\n",
    "from lib.numpy_pack import packArray,unpackArray\n",
    "\n",
    "#from lib.Eigen_decomp import Eigen_decomp\n",
    "from lib.YearPlotter import YearPlotter\n",
    "from lib.decomposer import *\n",
    "from lib.Reconstruction_plots import *\n",
    "\n",
    "\n",
    "from lib.import_modules import import_modules,modules\n",
    "import_modules(modules)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,widgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "print('version of ipwidgets=',widgets.__version__)\n",
    "\n",
    "import warnings  # Suppress Warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:24:24.574891Z",
     "start_time": "2018-04-26T22:24:21.951364Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[3]) created by __init__ at <ipython-input-6-61dfdb180541>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-61dfdb180541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#sc.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[3]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lib/numpy_pack.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/spark_PCA.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/computeStatistics.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/Reconstruction_plots.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/decomposer.py'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-latest/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark-latest/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 296\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[3]) created by __init__ at <ipython-input-6-61dfdb180541>:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py','lib/Reconstruction_plots.py','lib/decomposer.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Read Statistics File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:24:56.181108Z",
     "start_time": "2018-04-26T22:24:56.108652Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%state\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "measurements=STAT.keys()\n",
    "print('keys from STAT=',measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:24:57.600057Z",
     "start_time": "2018-04-26T22:24:57.597175Z"
    }
   },
   "outputs": [],
   "source": [
    "EigVec=STAT[m]['eigvec']\n",
    "Mean=STAT[m]['Mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Read data file into a spark DataFrame\n",
    "We focus on the snow-depth records, because the eigen-vectors for them make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:03.335857Z",
     "start_time": "2018-04-26T22:24:59.412606Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "filename=data_dir+'/%s.parquet'%state\n",
    "df_in=sqlContext.read.parquet(filename)\n",
    "#filter in \n",
    "df=df_in.filter(df_in.Measurement==m)\n",
    "df=df.drop('State')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing decomposition for each row, and add columns for coefficients and residuals\n",
    "\n",
    "Residuals are the remainder left after successive approximations:  \n",
    "1) Original vector = $\\vec{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) $\\vec{r}_0=\\vec{v}-\\vec{\\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) $\\vec{r}_1=\\vec{r}_0-(\\vec{v}\\cdot \\vec{u}_1) \\vec{u}_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) $\\vec{r}_2=\\vec{r}_1-(\\vec{v}\\cdot \\vec{u}_2) \\vec{u}_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) $\\vec{r}_3=\\vec{r}_0-(\\vec{v}\\cdot \\vec{u}_3) \\vec{u}_3$  \n",
    "6) ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each reidual $\\vec{r}_i$ we compute it's square norm, which we will refer to as **residual norm** :\n",
    "$$\\|\\vec{r}_i\\|_2^2 = \\sum_{j=1}^n (r_{i,j})^2$$  \n",
    "The smaller tha norm, the better the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A few things we know from linear algebra:\n",
    "\n",
    "1) The zero'th residual norm is the square distance of $\\vec{v}$ from the mean $\\vec{\\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) The $k$'th residual norm is the minimal square between $\\vec{v}$ and a point that can be exspressed as\n",
    "$$ \\vec{w}_k = \\vec{\\mu} + \\sum_{i=1}^k c_i \\vec{u}_i$$\n",
    "Where $c_1,\\ldots,c_k$ are arbitrary real numbers. We call $\\vec{w}_k$ the $k$'th approximation or reconstruction of $\\vec{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) The residual norms are non-increasing.  \n",
    "4) The residual vector $\\vec{r}_n$ is the zero vector. In other words, $\\vec{w}_n = \\vec{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`decompose_dataframe` axtracts the series from the row, computes the `k` to decomposition coefficients and \n",
    "the square norm of the residuals and constructs a new row that is reassembled into a new dataframe.  \n",
    "\n",
    "For more details, use `%load lib/decomposer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:13.113682Z",
     "start_time": "2018-04-26T22:25:12.403914Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k=5\n",
    "df2=decompose_dataframe(sqlContext,df,EigVec[:,:k],Mean).cache() # Make it possible to generate only first k coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:21.461528Z",
     "start_time": "2018-04-26T22:25:17.260834Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Join decomposition information with station information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:21.606940Z",
     "start_time": "2018-04-26T22:25:21.463862Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:21.703680Z",
     "start_time": "2018-04-26T22:25:21.609882Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "stations_df=sqlContext.read.parquet(data_dir+'/stations.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:24.641704Z",
     "start_time": "2018-04-26T22:25:24.524550Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stations_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:26.269541Z",
     "start_time": "2018-04-26T22:25:25.922716Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "jdf=df2.join(stations_df,on='Station',how='left')\n",
    "jdf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Removing years with little snow\n",
    "In some locations in NY and in some year, there is almost no snow accumulation. We want to treat these separately.\n",
    "\n",
    "To do so we compare the error of using the average to the error of using a zero vector. We keep only those yearXstation where the mean is a better approximation than the zero Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:28.982911Z",
     "start_time": "2018-04-26T22:25:28.531750Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#filter out vectors for which the mean is a worse approximation than zero.\n",
    "print('all Rows',jdf.count())\n",
    "df3=jdf.filter(jdf.res_mean<1)\n",
    "print('Rows where mean is better approx than zero',df3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Saving the decomposition in a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:31.239989Z",
     "start_time": "2018-04-26T22:25:31.000676Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "filename=data_dir+'/decon_'+state+'_'+m+'.parquet'\n",
    "!rm -rf $filename\n",
    "df3.write.parquet(filename)\n",
    "\n",
    "!du -sh $data_dir/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plot mean and top eigenvectors\n",
    "\n",
    "Construct approximations of a time series using the mean and the $k$ top eigen-vectors\n",
    "First, we plot the mean and the top $k$ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T23:41:39.943633Z",
     "start_time": "2018-04-24T23:41:27.348Z"
    },
    "hide_input": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,10));\n",
    "k=3\n",
    "EigVec=np.array(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])\n",
    "fig.savefig('r_figures/SNWD_mean_eigs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## plot Percentage of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:55.163096Z",
     "start_time": "2018-04-26T22:25:54.877825Z"
    },
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#  x=0 in the graphs below correspond to the fraction of the variance explained by the mean alone\n",
    "#  x=1,2,3,... are the residuals for eig1, eig1+eig2, eig1+eig2+eig3 ...\n",
    "fig,ax=plt.subplots(1,1);\n",
    "eigvals=STAT[m]['eigval']; eigvals/=sum(eigvals); cumvar=np.cumsum(eigvals); cumvar=100*np.insert(cumvar,0,0)\n",
    "ax.plot(cumvar[:10]); \n",
    "ax.grid(); \n",
    "ax.set_ylabel('Percent of variance explained')\n",
    "ax.set_xlabel('number of eigenvectors')\n",
    "ax.set_title('Percent of variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exploring the decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuitive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:25:58.268418Z",
     "start_time": "2018-04-26T22:25:58.258860Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#combine mean with Eigvecs and scale to similar range.\n",
    "print(EigVec.shape)\n",
    "_norm_Mean=Mean/max(Mean)*0.2\n",
    "A=[_norm_Mean]+[EigVec[:,i] for i in range(EigVec.shape[1])]\n",
    "Combined=np.stack(A).transpose()\n",
    "Combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:00.986344Z",
     "start_time": "2018-04-26T22:26:00.825307Z"
    },
    "hide_input": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig,axes=plt.subplots(1,1, sharex='col', sharey='row',figsize=(10,5));\n",
    "k=3\n",
    "EigVec=np.array(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "#YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "YearPlotter().plot(Combined,fig,axes,title=m+' Eigs',labels=['Mean']+['eig'+str(i+1) for i in range(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Eig1** is very similar to the Mean --- Indicates heavy/light snow\n",
    "* If **coef_1** is large: snow accumulation is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Eig2** is positive january, negative march. Indicates early vs. late season\n",
    "* If **coef_2** is high: snow season is early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Eig3** is positive Feb, negative Jan, March -- Indicates a short or long season.\n",
    "* If **Coef_3** is high: Season is short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Studying the effect of Coefficient 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:51:02.149579Z",
     "start_time": "2018-04-26T22:50:59.955092Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_2<0.1).sort(df3.coeff_2)\n",
    "print(df4.count())\n",
    "all_rows=df4.collect()\n",
    "rows=all_rows[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:13.922845Z",
     "start_time": "2018-04-26T22:26:13.698401Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking that res_2 is smaller than 0.1 and that rows are sorted based on coeff_2\n",
    "df4.select('coeff_1','coeff_2','coeff_3','res_1','res_2','res_3',).show(n=4,truncate=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:17.846015Z",
     "start_time": "2018-04-26T22:26:15.601628Z"
    },
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(all_rows[:12],Mean,EigVec)\n",
    "savefig('r_figures/SNWD_grid_negative_coeff_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:22.136417Z",
     "start_time": "2018-04-26T22:26:19.881866Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(all_rows[-12:],Mean,EigVec)\n",
    "savefig('r_figures/SNWD_grid_positive_coeff_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Studying the effect of Coefficient 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:26.310747Z",
     "start_time": "2018-04-26T22:26:23.153054Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.1).sort(df3.coeff_3)\n",
    "print(df4.count())\n",
    "all_rows=df4.collect()\n",
    "rows=all_rows[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:28.605866Z",
     "start_time": "2018-04-26T22:26:26.312806Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(all_rows[:12],Mean,EigVec)\n",
    "savefig('r_figures/SNWD_grid_negative_coeff_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:31.783101Z",
     "start_time": "2018-04-26T22:26:29.832911Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(all_rows[-12:],Mean,EigVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:41.107045Z",
     "start_time": "2018-04-26T22:26:33.728896Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.sort(df3.res_3)\n",
    "print(df4.count())\n",
    "all_rows=df4.collect()\n",
    "df4.select('coeff_1','coeff_2','coeff_3','res_3').show(n=4,truncate=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Fit\n",
    "\n",
    "First, lets plot the SNWD sequences which are best approximated using the first three eigen-vectors.\n",
    "\n",
    "In other words, the sequences for which the third residual is smallest.\n",
    "\n",
    "We can think of these as **architypical** sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:26:57.594309Z",
     "start_time": "2018-04-26T22:26:55.609670Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(all_rows[:12],Mean,EigVec,header='res_3=%3.2f', params=('res_3',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## worst fit\n",
    "\n",
    "Next, lets look at the sequence whose third residual is largest.\n",
    "\n",
    "We can think of those as **outliers** or **noise**. These seuqnces do not fit our model. \n",
    "\n",
    "Have many of these outliers is a problem: we are either getting poor data, or else our model is inadequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:11.129593Z",
     "start_time": "2018-04-26T22:27:09.262542Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_rows=all_rows[-4:]+all_rows[-504:-500]+all_rows[-1004:-1000]\n",
    "plot_recon_grid(bad_rows,Mean,EigVec,header='res_3=%3.2f', params=('res_3',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something to try\n",
    "Clearly, the majority of the poor fits are a result of undefined entries in the data.  \n",
    "Can you change the command to focus on years where most of the measurements are defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interactive plot of reconstruction\n",
    "\n",
    "Following is an interactive widget which lets you change the coefficients of the eigen-vectors to see the effect on the approximation.\n",
    "The initial state of the sliders (in the middle) corresponds to the optimal setting. You can zero a positive coefficient by moving the slider all the way down, zero a negative coefficient by moving it all the way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:51:50.667707Z",
     "start_time": "2018-04-26T22:51:50.623874Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "row=all_rows[-6]\n",
    "target=np.array(unpackArray(row.Values,np.float16),dtype=np.float64)\n",
    "eigen_decomp=Eigen_decomp(None,target,Mean,EigVec)\n",
    "total_var,residuals,coeff=eigen_decomp.compute_var_explained()\n",
    "res=residuals[1]\n",
    "print('residual normalized norm  after mean:',res[0])\n",
    "print('residual normalized norm  after mean + top eigs:',res[1:])\n",
    "\n",
    "plotter=recon_plot(eigen_decomp,year_axis=True,interactive=True)\n",
    "display(plotter.get_Interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying the distribution of the coefficients.\n",
    "\n",
    "To answer this question we extract all of the values of `res_3` which is the residual variance after the Mean and the \n",
    "first two Eigen-vectors have been subtracted out. We rely here on the fact that `df3` is already sorted according to `res_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:36.000699Z",
     "start_time": "2018-04-26T22:27:35.784268Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf=df3.select(['Station','Year','coeff_1','coeff_2','coeff_3','res_1','res_2','res_3','res_mean','total_var']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:36.938272Z",
     "start_time": "2018-04-26T22:27:36.934271Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:37.936583Z",
     "start_time": "2018-04-26T22:27:37.934188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pdf=pdf.set_index('Year')\n",
    "#pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:40.237596Z",
     "start_time": "2018-04-26T22:27:38.805549Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf[['Year','coeff_3']][pdf['Year']>1950].boxplot(by='Year',figsize=[15,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:47.430414Z",
     "start_time": "2018-04-26T22:27:47.198461Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf.plot.scatter('coeff_1','coeff_2',figsize=[15,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:55.626265Z",
     "start_time": "2018-04-26T22:27:55.549124Z"
    }
   },
   "outputs": [],
   "source": [
    "grpby=pdf.groupby('Year')['coeff_1']\n",
    "ratio=grpby.mean()/grpby.std()\n",
    "np.nanmax(ratio),np.nanmin(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:27:57.972547Z",
     "start_time": "2018-04-26T22:27:57.965533Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function for plotting the CDF of a given feature\n",
    "def plot_CDF(feat):\n",
    "    rows=df4.select(feat).sort(feat).collect()\n",
    "    vals=[r[feat] for r in rows]\n",
    "    P=np.arange(0,1,1./(len(vals)+1))\n",
    "    vals=[vals[0]]+vals\n",
    "    axis.plot(vals,P,label=feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:28:00.755324Z",
     "start_time": "2018-04-26T22:28:00.749488Z"
    }
   },
   "outputs": [],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:28:49.903432Z",
     "start_time": "2018-04-26T22:28:01.693020Z"
    }
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10,8))\n",
    "axis=gca()\n",
    "\n",
    "#plot_CDF('res_mean') # why does this not fit?\n",
    "plot_CDF('res_1')\n",
    "plot_CDF('res_2')\n",
    "plot_CDF('res_3')\n",
    "plot_CDF('res_4')\n",
    "plot_CDF('res_5')\n",
    "ylabel(' of instances')\n",
    "xlabel('Residual')\n",
    "grid()\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T22:29:01.726088Z",
     "start_time": "2018-04-26T22:28:49.904935Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_CDF('coeff_1')\n",
    "savefig('r_figures/SNWD_coeff_3_CDF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T23:41:39.969971Z",
     "start_time": "2018-04-24T23:41:28.257Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_CDF('coeff_2')\n",
    "savefig('r_figures/SNWD_coeff_3_CDF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T23:41:39.971160Z",
     "start_time": "2018-04-24T23:41:28.261Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_CDF('coeff_3')\n",
    "savefig('r_figures/SNWD_coeff_3_CDF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T23:41:39.972341Z",
     "start_time": "2018-04-24T23:41:28.263Z"
    }
   },
   "outputs": [],
   "source": [
    "filename=data_dir+'/recon_'+state+'_'+m+'.parquet'\n",
    "!rm -rf $filename\n",
    "df3.write.parquet(filename)\n",
    "\n",
    "!du -sh $data_dir/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "0d4726d074414304b7910c9bc9aee2a0": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "9dfb4bbaf8664891a93b62da7476d8fe": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
