{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local files\n",
    "\n",
    "### Download and Uplaod\n",
    "\n",
    "Download and upload files via the Spark Notebook interface.\n",
    "\n",
    "### Access Local Files\n",
    "\n",
    "The file path to local files requires `file://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ls /etc/passwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s3helper\n",
    "\n",
    "The object `s3helper` is a tool to transfer files between local filesystem, HDFS and S3.\n",
    "\n",
    "Run `s3helper.help()` to learn all its methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s3helper.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Copy meta information files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket('mas-dse-open',region='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_files=s3helper.ls('Weather/Info')\n",
    "Without_path=[x[13:] for x in info_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir /mnt/workspace/WeatherInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Weather/Info/README.md to /mnt/workspace/WeatherInfo/README.md\n",
      "From Weather/Info/US_stations.tsv.gz to /mnt/workspace/WeatherInfo/US_stations.tsv.gz\n",
      "From Weather/Info/all_stations.tsv.gz to /mnt/workspace/WeatherInfo/all_stations.tsv.gz\n",
      "From Weather/Info/data-source.txt to /mnt/workspace/WeatherInfo/data-source.txt\n",
      "From Weather/Info/dist2coast.txt.gz to /mnt/workspace/WeatherInfo/dist2coast.txt.gz\n",
      "From Weather/Info/ghcnd-countries.txt to /mnt/workspace/WeatherInfo/ghcnd-countries.txt\n",
      "From Weather/Info/ghcnd-readme.txt to /mnt/workspace/WeatherInfo/ghcnd-readme.txt\n",
      "From Weather/Info/ghcnd-states.txt to /mnt/workspace/WeatherInfo/ghcnd-states.txt\n",
      "From Weather/Info/ghcnd-stations_buffered.txt to /mnt/workspace/WeatherInfo/ghcnd-stations_buffered.txt\n",
      "From Weather/Info/ghcnd-version.txt to /mnt/workspace/WeatherInfo/ghcnd-version.txt\n",
      "From Weather/Info/stations_projections.pickle to /mnt/workspace/WeatherInfo/stations_projections.pickle\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(info_files)-1):   # Last entry is the directory name\n",
    "    _from=info_files[i]\n",
    "    _to='/mnt/workspace/WeatherInfo/'+Without_path[i]\n",
    "    print('From %s to %s'%(_from,_to))\n",
    "    s3helper.s3_to_local(_from,_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to original information\n",
    "\n",
    "Looking in README.md, you see that all of the data can be downloaded from the noaa web site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %load /mnt/workspace/WeatherInfo/README.md\n",
    "## Meta-data for weather data collection\n",
    "\n",
    "### Original info files\n",
    "#Downloaded from ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/  \n",
    "\n",
    "* 2016-04-13 13:58:29        270 `ghcnd-version.txt`\n",
    "* 2016-04-13 13:58:26        218 `data-source.txt`\n",
    "* 2016-04-13 13:58:27      22422 `ghcnd-readme.txt`  : Main documentation.\n",
    "* 2016-04-13 13:58:28    7760844 `ghcnd-stations_buffered.txt`\n",
    "\n",
    "### Compliled tables\n",
    "* 2018-03-15 08:52:40    1519539 `all_stations.tsv.gz` : a tab-separated version of `ghcnd-stations_buffered.txt`\n",
    "* 2018-03-15 08:52:16     328590 `US_stations.tsv.gz` : a tab separated list of stations in the US that have a sufficient number of measurement. Includes also distance to shoreline for station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %load /mnt/workspace/WeatherInfo/ghcnd-readme.txt\n",
    "README FILE FOR DAILY GLOBAL HISTORICAL CLIMATOLOGY NETWORK (GHCN-DAILY) \n",
    "Version 3.00\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "I. DOWNLOAD QUICK START\n",
    "\n",
    "Start by downloading \"ghcnd-stations.txt,\" which has metadata for all stations.\n",
    "\n",
    "Then download one of the following TAR files:\n",
    "\n",
    "  - \"ghcnd-all.tar.gz\" if you want all of GHCN-Daily, OR\n",
    "  - \"ghcnd-gsn.tar.gz\" if you only want the GCOS Surface Network (GSN), OR\n",
    "  - \"ghcnd-hcn.tar.gz\" if you only want the U.S. Historical Climatology Network \n",
    "    (U.S. HCN).\n",
    "\n",
    "Then uncompress and untar the contents of the tar file, \n",
    "e.g., by using the following Linux command:\n",
    "\n",
    "tar xzvf ghcnd_xxx.tar.gz\n",
    "\n",
    "Where \"xxx\" stands for \"all\", \"hcn\", or \"gsn\" as applicable. The files will be \n",
    "extracted into a subdirectory under the directory where the command is issued.\n",
    "\n",
    "ALTERNATIVELY, if you only need data for one station:\n",
    "\n",
    "  - Find the station's name in \"ghcnd-stations.txt\" and note its station\n",
    "    identification code (e.g., PHOENIX AP (Airport) is \"USW00023183\"); and\n",
    "  - Download the data file (i.e., \".dly\" file) that corresponds to this code\n",
    "    (e.g., \"USW00023183.dly\" has the data for PHOENIX AP).  \n",
    "    Note that the \".dly\" file is located in the \"all\" subdirectory.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "II. CONTENTS OF ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily\n",
    "\n",
    "all:                  Directory with \".dly\" files for all of GHCN-Daily\n",
    "gsn:                  Directory with \".dly\" files for the GCOS Surface Network \n",
    "                     (GSN)\n",
    "hcn:                  Directory with \".dly\" files for U.S. HCN\n",
    "by_year:              Directory with GHCN Daily files parsed into yearly\n",
    "                      subsets with observation times where available.  See the\n",
    "\t\t      /by_year/readme.txt and \n",
    "\t\t      /by_year/ghcn-daily-by_year-format.rtf \n",
    "\t\t      files for further information\n",
    "grid:\t              Directory with the GHCN-Daily gridded dataset known \n",
    "                      as HadGHCND\n",
    "papers:\t\t      Directory with pdf versions of journal articles relevant \n",
    "                      to the GHCN-Daily dataset\n",
    "figures:\t      Directory containing figures that summarize the inventory \n",
    "                      of GHCN-Daily station records\t\t    \n",
    "\n",
    "ghcnd-all.tar.gz:  TAR file of the GZIP-compressed files in the \"all\" directory\n",
    "ghcnd-gsn.tar.gz:  TAR file of the GZIP-compressed \"gsn\" directory\n",
    "ghcnd-hcn.tar.gz:  TAR file of the GZIP-compressed \"hcn\" directory\n",
    "\n",
    "ghcnd-countries.txt:  List of country codes (FIPS) and names\n",
    "ghcnd-inventory.txt:  File listing the periods of record for each station and \n",
    "                      element\n",
    "ghcnd-stations.txt:   List of stations and their metadata (e.g., coordinates)\n",
    "ghcnd-states.txt:     List of U.S. state and Canadian Province codes \n",
    "                      used in ghcnd-stations.txt\n",
    "ghcnd-version.txt:    File that specifies the current version of GHCN Daily\n",
    "\n",
    "readme.txt:           This file\n",
    "status.txt:           Notes on the current status of GHCN-Daily\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "III. FORMAT OF DATA FILES (\".dly\" FILES)\n",
    "\n",
    "Each \".dly\" file contains data for one station.  The name of the file\n",
    "corresponds to a station's identification code.  For example, \"USC00026481.dly\"\n",
    "contains the data for the station with the identification code USC00026481).\n",
    "\n",
    "Each record in a file contains one month of daily data.  The variables on each\n",
    "line include the following:\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "YEAR         12-15   Integer\n",
    "MONTH        16-17   Integer\n",
    "ELEMENT      18-21   Character\n",
    "VALUE1       22-26   Integer\n",
    "MFLAG1       27-27   Character\n",
    "QFLAG1       28-28   Character\n",
    "SFLAG1       29-29   Character\n",
    "VALUE2       30-34   Integer\n",
    "MFLAG2       35-35   Character\n",
    "QFLAG2       36-36   Character\n",
    "SFLAG2       37-37   Character\n",
    "  .           .          .\n",
    "  .           .          .\n",
    "  .           .          .\n",
    "VALUE31    262-266   Integer\n",
    "MFLAG31    267-267   Character\n",
    "QFLAG31    268-268   Character\n",
    "SFLAG31    269-269   Character\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "ID         is the station identification code.  Please see \"ghcnd-stations.txt\"\n",
    "           for a complete list of stations and their metadata.\n",
    "YEAR       is the year of the record.\n",
    "\n",
    "MONTH      is the month of the record.\n",
    "\n",
    "ELEMENT    is the element type.   There are five core elements as well as a number\n",
    "           of addition elements.  \n",
    "\t   \n",
    "\t   The five core elements are:\n",
    "\n",
    "           PRCP = Precipitation (tenths of mm)\n",
    "   \t   SNOW = Snowfall (mm)\n",
    "\t   SNWD = Snow depth (mm)\n",
    "           TMAX = Maximum temperature (tenths of degrees C)\n",
    "           TMIN = Minimum temperature (tenths of degrees C)\n",
    "\t   \n",
    "\t   The other elements are:\n",
    "\t   \n",
    "\t   ACMC = Average cloudiness midnight to midnight from 30-second \n",
    "\t          ceilometer data (percent)\n",
    "\t   ACMH = Average cloudiness midnight to midnight from \n",
    "\t          manual observations (percent)\n",
    "           ACSC = Average cloudiness sunrise to sunset from 30-second \n",
    "\t          ceilometer data (percent)\n",
    "\t   ACSH = Average cloudiness sunrise to sunset from manual \n",
    "\t          observations (percent)\n",
    "\t   AWND = Average daily wind speed (tenths of meters per second)\n",
    "\t   DAEV = Number of days included in the multiday evaporation\n",
    "\t          total (MDEV)\n",
    "\t   DAPR = Number of days included in the multiday precipiation \n",
    "\t          total (MDPR)\n",
    "           DASF = Number of days included in the multiday snowfall \n",
    "\t          total (MDSF)\t\t  \n",
    "\t   DATN = Number of days included in the multiday minimum temperature \n",
    "\t         (MDTN)\n",
    "\t   DATX = Number of days included in the multiday maximum temperature \n",
    "\t          (MDTX)\n",
    "           DAWM = Number of days included in the multiday wind movement\n",
    "\t          (MDWM)\n",
    "\t   DWPR = Number of days with non-zero precipitation included in \n",
    "\t          multiday precipitation total (MDPR)\n",
    "\t   EVAP = Evaporation of water from evaporation pan (tenths of mm)\n",
    "\t   FMTM = Time of fastest mile or fastest 1-minute wind \n",
    "\t          (hours and minutes, i.e., HHMM)\n",
    "\t   FRGB = Base of frozen ground layer (cm)\n",
    "\t   FRGT = Top of frozen ground layer (cm)\n",
    "\t   FRTH = Thickness of frozen ground layer (cm)\n",
    "\t   GAHT = Difference between river and gauge height (cm)\n",
    "\t   MDEV = Multiday evaporation total (tenths of mm; use with DAEV)\n",
    "\t   MDPR = Multiday precipitation total (tenths of mm; use with DAPR and \n",
    "\t          DWPR, if available)\n",
    "\t   MDSF = Multiday snowfall total \n",
    "\t   MDTN = Multiday minimum temperature (tenths of degrees C; use with \n",
    "\t          DATN)\n",
    "\t   MDTX = Multiday maximum temperature (tenths of degress C; use with \n",
    "\t          DATX)\n",
    "\t   MDWM = Multiday wind movement (km)\n",
    "           MNPN = Daily minimum temperature of water in an evaporation pan \n",
    "\t         (tenths of degrees C)\n",
    "           MXPN = Daily maximum temperature of water in an evaporation pan \n",
    "\t         (tenths of degrees C)\n",
    "\t   PGTM = Peak gust time (hours and minutes, i.e., HHMM)\n",
    "\t   PSUN = Daily percent of possible sunshine (percent)\n",
    "\t   SN*# = Minimum soil temperature (tenths of degrees C)\n",
    "\t          where * corresponds to a code\n",
    "\t          for ground cover and # corresponds to a code for soil \n",
    "\t\t  depth.  \n",
    "\t\t  \n",
    "\t\t  Ground cover codes include the following:\n",
    "\t\t  0 = unknown\n",
    "\t\t  1 = grass\n",
    "\t\t  2 = fallow\n",
    "\t\t  3 = bare ground\n",
    "\t\t  4 = brome grass\n",
    "\t\t  5 = sod\n",
    "\t\t  6 = straw multch\n",
    "\t\t  7 = grass muck\n",
    "\t\t  8 = bare muck\n",
    "\t\t  \n",
    "\t\t  Depth codes include the following:\n",
    "\t\t  1 = 5 cm\n",
    "\t\t  2 = 10 cm\n",
    "\t\t  3 = 20 cm\n",
    "\t\t  4 = 50 cm\n",
    "\t\t  5 = 100 cm\n",
    "\t\t  6 = 150 cm\n",
    "\t\t  7 = 180 cm\n",
    "\t\t  \n",
    "\t   SX*# = Maximum soil temperature (tenths of degrees C) \n",
    "\t          where * corresponds to a code for ground cover \n",
    "\t\t  and # corresponds to a code for soil depth. \n",
    "\t\t  See SN*# for ground cover and depth codes. \n",
    "\n",
    "           THIC = Thickness of ice on water (tenths of mm)\t\n",
    " \t   TOBS = Temperature at the time of observation (tenths of degrees C)\n",
    "\t   TSUN = Daily total sunshine (minutes)\n",
    "\t   WDF1 = Direction of fastest 1-minute wind (degrees)\n",
    "\t   WDF2 = Direction of fastest 2-minute wind (degrees)\n",
    "\t   WDF5 = Direction of fastest 5-second wind (degrees)\n",
    "\t   WDFG = Direction of peak wind gust (degrees)\n",
    "\t   WDFI = Direction of highest instantaneous wind (degrees)\n",
    "\t   WDFM = Fastest mile wind direction (degrees)\n",
    "           WDMV = 24-hour wind movement (km)\t   \n",
    "           WESD = Water equivalent of snow on the ground (tenths of mm)\n",
    "\t   WESF = Water equivalent of snowfall (tenths of mm)\n",
    "\t   WSF1 = Fastest 1-minute wind speed (tenths of meters per second)\n",
    "\t   WSF2 = Fastest 2-minute wind speed (tenths of meters per second)\n",
    "\t   WSF5 = Fastest 5-second wind speed (tenths of meters per second)\n",
    "\t   WSFG = Peak guest wind speed (tenths of meters per second)\n",
    "\t   WSFI = Highest instantaneous wind speed (tenths of meters per second)\n",
    "\t   WSFM = Fastest mile wind speed (tenths of meters per second)\n",
    "\t   WT** = Weather Type where ** has one of the following values:\n",
    "\t   \n",
    "                  01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "                  02 = Heavy fog or heaving freezing fog (not always \n",
    "\t\t       distinquished from fog)\n",
    "                  03 = Thunder\n",
    "                  04 = Ice pellets, sleet, snow pellets, or small hail \n",
    "                  05 = Hail (may include small hail)\n",
    "                  06 = Glaze or rime \n",
    "                  07 = Dust, volcanic ash, blowing dust, blowing sand, or \n",
    "\t\t       blowing obstruction\n",
    "                  08 = Smoke or haze \n",
    "                  09 = Blowing or drifting snow\n",
    "                  10 = Tornado, waterspout, or funnel cloud \n",
    "                  11 = High or damaging winds\n",
    "                  12 = Blowing spray\n",
    "                  13 = Mist\n",
    "                  14 = Drizzle\n",
    "                  15 = Freezing drizzle \n",
    "                  16 = Rain (may include freezing rain, drizzle, and\n",
    "\t\t       freezing drizzle) \n",
    "                  17 = Freezing rain \n",
    "                  18 = Snow, snow pellets, snow grains, or ice crystals\n",
    "                  19 = Unknown source of precipitation \n",
    "                  21 = Ground fog \n",
    "                  22 = Ice fog or freezing fog\n",
    "\t\t  \n",
    "            WV** = Weather in the Vicinity where ** has one of the following \n",
    "\t           values:\n",
    "\t\t   \n",
    "\t\t   01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "\t\t   03 = Thunder\n",
    "\t\t   07 = Ash, dust, sand, or other blowing obstruction\n",
    "\t\t   18 = Snow or ice crystals\n",
    "\t\t   20 = Rain or snow shower\n",
    "\t\t   \n",
    "VALUE1     is the value on the first day of the month (missing = -9999).\n",
    "\n",
    "MFLAG1     is the measurement flag for the first day of the month.  There are\n",
    "           ten possible values:\n",
    "\n",
    "           Blank = no measurement information applicable\n",
    "           B     = precipitation total formed from two 12-hour totals\n",
    "           D     = precipitation total formed from four six-hour totals\n",
    "\t   H     = represents highest or lowest hourly temperature\n",
    "\t   K     = converted from knots \n",
    "\t   L     = temperature appears to be lagged with respect to reported\n",
    "\t           hour of observation \n",
    "           O     = converted from oktas \n",
    "\t   P     = identified as \"missing presumed zero\" in DSI 3200 and 3206\n",
    "           T     = trace of precipitation, snowfall, or snow depth\n",
    "\t   W     = converted from 16-point WBAN code (for wind direction)\n",
    "\n",
    "QFLAG1     is the quality flag for the first day of the month.  There are \n",
    "           fourteen possible values:\n",
    "\n",
    "           Blank = did not fail any quality assurance check\n",
    "           D     = failed duplicate check\n",
    "           G     = failed gap check\n",
    "           I     = failed internal consistency check\n",
    "           K     = failed streak/frequent-value check\n",
    "\t   L     = failed check on length of multiday period \n",
    "           M     = failed megaconsistency check\n",
    "           N     = failed naught check\n",
    "           O     = failed climatological outlier check\n",
    "           R     = failed lagged range check\n",
    "           S     = failed spatial consistency check\n",
    "           T     = failed temporal consistency check\n",
    "           W     = temperature too warm for snow\n",
    "           X     = failed bounds check\n",
    "\t   Z     = flagged as a result of an official Datzilla \n",
    "\t           investigation\n",
    "\n",
    "SFLAG1     is the source flag for the first day of the month.  There are \n",
    "           twenty seven possible values (including blank, upper and \n",
    "\t   lower case letters):\n",
    "\n",
    "           Blank = No source (i.e., data value missing)\n",
    "           0     = U.S. Cooperative Summary of the Day (NCDC DSI-3200)\n",
    "           6     = CDMP Cooperative Summary of the Day (NCDC DSI-3206)\n",
    "           7     = U.S. Cooperative Summary of the Day -- Transmitted \n",
    "\t           via WxCoder3 (NCDC DSI-3207)\n",
    "           A     = U.S. Automated Surface Observing System (ASOS) \n",
    "                   real-time data (since January 1, 2006)\n",
    "\t   a     = Australian data from the Australian Bureau of Meteorology\n",
    "           B     = U.S. ASOS data for October 2000-December 2005 (NCDC \n",
    "                   DSI-3211)\n",
    "\t   b     = Belarus update\n",
    "\t   E     = European Climate Assessment and Dataset (Klein Tank \n",
    "\t           et al., 2002)\t   \n",
    "           F     = U.S. Fort data \n",
    "           G     = Official Global Climate Observing System (GCOS) or \n",
    "                   other government-supplied data\n",
    "           H     = High Plains Regional Climate Center real-time data\n",
    "           I     = International collection (non U.S. data received through\n",
    "\t           personal contacts)\n",
    "           K     = U.S. Cooperative Summary of the Day data digitized from\n",
    "\t           paper observer forms (from 2011 to present)\n",
    "           M     = Monthly METAR Extract (additional ASOS data)\n",
    "\t   N     = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)\n",
    "\t   Q     = Data from several African countries that had been \n",
    "\t           \"quarantined\", that is, withheld from public release\n",
    "\t\t   until permission was granted from the respective \n",
    "\t           meteorological services\n",
    "           R     = NCDC Reference Network Database (Climate Reference Network\n",
    "\t           and Historical Climatology Network-Modernized)\n",
    "\t   r     = All-Russian Research Institute of Hydrometeorological \n",
    "\t           Information-World Data Center\n",
    "           S     = Global Summary of the Day (NCDC DSI-9618)\n",
    "                   NOTE: \"S\" values are derived from hourly synoptic reports\n",
    "                   exchanged on the Global Telecommunications System (GTS).\n",
    "                   Daily values derived in this fashion may differ significantly\n",
    "                   from \"true\" daily data, particularly for precipitation\n",
    "                   (i.e., use with caution).\n",
    "           T     = SNOwpack TELemtry (SNOTEL) data obtained from the Western\n",
    "\t           Regional Climate Center\n",
    "\t   U     = Remote Automatic Weather Station (RAWS) data obtained\n",
    "\t           from the Western Regional Climate Center\t   \n",
    "\t   u     = Ukraine update\t   \n",
    "\t   W     = WBAN/ASOS Summary of the Day from NCDC's Integrated \n",
    "\t           Surface Data (ISD).  \n",
    "           X     = U.S. First-Order Summary of the Day (NCDC DSI-3210)\n",
    "\t   Z     = Datzilla official additions or replacements \n",
    "\t   z     = Uzbekistan update\n",
    "\t   \n",
    "\t   When data are available for the same time from more than one source,\n",
    "\t   the highest priority source is chosen according to the following\n",
    "\t   priority order (from highest to lowest):\n",
    "\t   Z,R,0,6,X,W,K,7,F,B,M,r,E,z,u,b,a,G,Q,I,A,N,H,S\n",
    "\t   \n",
    "\t   \n",
    "VALUE2     is the value on the second day of the month\n",
    "\n",
    "MFLAG2     is the measurement flag for the second day of the month.\n",
    "\n",
    "QFLAG2     is the quality flag for the second day of the month.\n",
    "\n",
    "SFLAG2     is the source flag for the second day of the month.\n",
    "\n",
    "... and so on through the 31st day of the month.  Note: If the month has less \n",
    "than 31 days, then the remaining variables are set to missing (e.g., for April, \n",
    "VALUE31 = -9999, MFLAG31 = blank, QFLAG31 = blank, SFLAG31 = blank).\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "IV. FORMAT OF \"ghcnd-stations.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSNFLAG      73-75   Character\n",
    "HCNFLAG      77-79   Character\n",
    "WMOID        81-85   Character\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "ID         is the station identification code.  Note that the first two\n",
    "           characters denote the FIPS  country code, the third character \n",
    "           is a network code that identifies the station numbering system \n",
    "           used, and the remaining eight characters contain the actual \n",
    "           station ID. \n",
    "\n",
    "           See \"ghcnd-countries.txt\" for a complete list of country codes.\n",
    "\t   See \"ghcnd-states.txt\" for a list of state/province/territory codes.\n",
    "\n",
    "           The network code  has the following five values:\n",
    "\n",
    "           0 = unspecified (station identified by up to eight \n",
    "\t       alphanumeric characters)\n",
    "\t   1 = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)\n",
    "\t       based identification number.  To ensure consistency with\n",
    "\t       with GHCN Daily, all numbers in the original CoCoRaHS IDs\n",
    "\t       have been left-filled to make them all four digits long. \n",
    "\t       In addition, the characters \"-\" and \"_\" have been removed \n",
    "\t       to ensure that the IDs do not exceed 11 characters when \n",
    "\t       preceded by \"US1\". For example, the CoCoRaHS ID \n",
    "\t       \"AZ-MR-156\" becomes \"US1AZMR0156\" in GHCN-Daily\n",
    "           C = U.S. Cooperative Network identification number (last six \n",
    "               characters of the GHCN-Daily ID)\n",
    "\t   E = Identification number used in the ECA&D non-blended\n",
    "\t       dataset\n",
    "\t   M = World Meteorological Organization ID (last five\n",
    "\t       characters of the GHCN-Daily ID)\n",
    "\t   N = Identification number used in data supplied by a \n",
    "\t       National Meteorological or Hydrological Center\n",
    "\t   R = U.S. Interagency Remote Automatic Weather Station (RAWS)\n",
    "\t       identifier\n",
    "\t   S = U.S. Natural Resources Conservation Service SNOwpack\n",
    "\t       TELemtry (SNOTEL) station identifier\n",
    "           W = WBAN identification number (last five characters of the \n",
    "               GHCN-Daily ID)\n",
    "\n",
    "LATITUDE   is latitude of the station (in decimal degrees).\n",
    "\n",
    "LONGITUDE  is the longitude of the station (in decimal degrees).\n",
    "\n",
    "ELEVATION  is the elevation of the station (in meters, missing = -999.9).\n",
    "\n",
    "\n",
    "STATE      is the U.S. postal code for the state (for U.S. stations only).\n",
    "\n",
    "NAME       is the name of the station.\n",
    "\n",
    "GSNFLAG    is a flag that indicates whether the station is part of the GCOS\n",
    "           Surface Network (GSN). The flag is assigned by cross-referencing \n",
    "           the number in the WMOID field with the official list of GSN \n",
    "           stations. There are two possible values:\n",
    "\n",
    "           Blank = non-GSN station or WMO Station number not available\n",
    "           GSN   = GSN station \n",
    "\n",
    "HCNFLAG    is a flag that indicates whether the station is part of the U.S.\n",
    "           Historical Climatology Network (HCN).  There are two possible values:\n",
    "\n",
    "           Blank = non-HCN station\n",
    "           HCN   = HCN station\n",
    "\n",
    "WMOID      is the World Meteorological Organization (WMO) number for the\n",
    "           station.  If the station has no WMO number, then the field is blank.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "V. FORMAT OF \"ghcnd-countries.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "CODE          1-2    Character\n",
    "NAME         4-50    Character\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "CODE       is the FIPS country code of the country where the station is \n",
    "           located (from FIPS Publication 10-4 at \n",
    "           www.cia.gov/cia/publications/factbook/appendix/appendix-d.html).\n",
    "\n",
    "NAME       is the name of the country.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "VI. FORMAT OF \"ghcnd-states.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "CODE          1-2    Character\n",
    "NAME         4-50    Character\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "CODE       is the POSTAL code of the U.S. state/territory or Canadian \n",
    "           province where the station is located \n",
    "\n",
    "NAME       is the name of the state, territory or province.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "VII. FORMAT OF \"ghcnd-inventory.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEMENT      32-35   Character\n",
    "FIRSTYEAR    37-40   Integer\n",
    "LASTYEAR     42-45   Integer\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "ID         is the station identification code.  Please see \"ghcnd-stations.txt\"\n",
    "           for a complete list of stations and their metadata.\n",
    "\n",
    "LATITUDE   is the latitude of the station (in decimal degrees).\n",
    "\n",
    "LONGITUDE  is the longitude of the station (in decimal degrees).\n",
    "\n",
    "ELEMENT    is the element type.  See section III for a definition of elements.\n",
    "\n",
    "FIRSTYEAR  is the first year of unflagged data for the given element.\n",
    "\n",
    "LASTYEAR   is the last year of unflagged data for the given element.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "VIII.  REFERENCES\n",
    "\n",
    "Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface\n",
    "air temperature and precipitation series for the European Climate Assessment.\n",
    "Int. J. of Climatol., 22, 1441-1453.\n",
    "Data and metadata available at http://eca.knmi.nl\n",
    "\n",
    "\n",
    "\n",
    "For additional information, please send an e-mail to ncdc.ghcnd@noaa.gov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Main data bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'.aws',\n",
       " u'.bash_profile',\n",
       " u'.bashrc',\n",
       " u'.conda',\n",
       " u'.ipython',\n",
       " u'.jupyter',\n",
       " u'.local',\n",
       " u'.ssh',\n",
       " u'ALL.csv.gz',\n",
       " u'ALLBootstrap.sh',\n",
       " u'MasterBootstrap.sh',\n",
       " u'NY.parquet',\n",
       " u'PrivateBootstrap.sh',\n",
       " u'RunFromTerminal.sh',\n",
       " u'US_Weather_with_smoothed.parquet',\n",
       " u'US_Weather_with_smoothed.parquet_$folder$',\n",
       " u'US_stations.parquet',\n",
       " u'US_weather.parquet',\n",
       " u'fromLocal',\n",
       " u'info',\n",
       " u's3helper.py',\n",
       " u's3hook.sh',\n",
       " u'weather.parquet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.open_bucket('dse-weather-west-2', region=\"us-west-2\")\n",
    "s3helper.ls('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) List files in the S3 bucket and HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list all files in the root directory of the bucket\n",
      ".aws\n",
      ".bash_profile\n",
      ".bashrc\n",
      ".conda\n",
      ".ipython\n",
      ".jupyter\n",
      ".local\n",
      ".ssh\n",
      "ALL.csv.gz\n",
      "ALLBootstrap.sh\n",
      "MasterBootstrap.sh\n",
      "NY.parquet\n",
      "PrivateBootstrap.sh\n",
      "RunFromTerminal.sh\n",
      "US_Weather_with_smoothed.parquet\n",
      "US_Weather_with_smoothed.parquet_$folder$\n",
      "US_stations.parquet\n",
      "US_weather.parquet\n",
      "fromLocal\n",
      "info\n",
      "s3helper.py\n",
      "s3hook.sh\n",
      "weather.parquet\n",
      "\n",
      "List files in directory \"fromHDFS\"\n",
      "[]\n",
      "\n",
      "List files in root of hdfs\n",
      "Found 3 items\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-05-15 03:33 /tmp\n",
      "drwxr-xr-x   - hdfs hadoop          0 2020-05-15 03:33 /user\n",
      "drwxr-xr-x   - hdfs hadoop          0 2020-05-15 03:33 /var\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('list all files in the root directory of the bucket')\n",
    "print(\"\\n\".join(s3helper.ls_s3()))\n",
    "print('\\nList files in directory \"fromHDFS\"')\n",
    "print(s3helper.ls_s3('fromHDFS'))\n",
    "print('\\nList files in root of hdfs')\n",
    "print(s3helper.ls_hdfs());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Move files around local filesystem, HDFS and S3\n",
    "\n",
    "As described in `s3helper.help()`, there are five methods for file transfers:\n",
    "\n",
    "1. `s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)`\n",
    "2. `s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)`\n",
    "3. `s3helper.s3_to_local(<s3_file_path>, <local_file_path>)`\n",
    "4. `s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)`\n",
    "5. `s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.local_to_s3(\"/home/hadoop/s3helper.py\", \"fromLocal/s3helper.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fromLocal/s3helper.py']\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_s3(\"fromLocal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move data from S3 to HDFS to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/05/15 03:37:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/_SUCCESS' for reading\n",
      "20/05/15 03:37:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00000-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00001-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:22 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00002-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:24 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00003-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:25 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00004-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:25 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00005-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:26 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00006-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:27 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00007-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:28 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00008-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:29 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00009-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:30 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00010-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:32 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00011-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:33 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00012-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:34 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00013-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:35 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00014-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:36 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00015-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00016-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:38 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00017-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:40 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00018-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:41 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00019-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:42 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00020-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:43 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00021-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00022-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:46 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00023-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:47 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00024-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:48 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00025-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:49 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00026-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:50 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00027-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:51 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00028-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:52 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00029-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:53 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00030-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:55 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00031-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:56 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00032-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:57 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00033-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:58 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00034-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:37:59 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00035-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:01 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00036-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:02 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00037-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:03 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00038-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:04 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00039-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:06 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00040-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:07 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00041-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:08 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00042-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:09 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00043-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:11 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00044-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00045-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00046-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:15 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00047-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:16 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00048-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:17 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00049-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00050-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00051-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00052-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:22 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00053-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:24 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00054-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:25 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00055-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:26 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00056-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "20/05/15 03:38:28 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00057-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2020-05-15 03:37 /tmp/weather.parquet/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop   40670401 2020-05-15 03:37 /tmp/weather.parquet/part-00000-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40307528 2020-05-15 03:37 /tmp/weather.parquet/part-00001-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40016618 2020-05-15 03:37 /tmp/weather.parquet/part-00002-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40377232 2020-05-15 03:37 /tmp/weather.parquet/part-00003-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40119938 2020-05-15 03:37 /tmp/weather.parquet/part-00004-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40278884 2020-05-15 03:37 /tmp/weather.parquet/part-00005-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40078149 2020-05-15 03:37 /tmp/weather.parquet/part-00006-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40838598 2020-05-15 03:37 /tmp/weather.parquet/part-00007-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40257133 2020-05-15 03:37 /tmp/weather.parquet/part-00008-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40970219 2020-05-15 03:37 /tmp/weather.parquet/part-00009-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39737699 2020-05-15 03:37 /tmp/weather.parquet/part-00010-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40588195 2020-05-15 03:37 /tmp/weather.parquet/part-00011-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39670806 2020-05-15 03:37 /tmp/weather.parquet/part-00012-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40265820 2020-05-15 03:37 /tmp/weather.parquet/part-00013-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40504283 2020-05-15 03:37 /tmp/weather.parquet/part-00014-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39839877 2020-05-15 03:37 /tmp/weather.parquet/part-00015-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39666134 2020-05-15 03:37 /tmp/weather.parquet/part-00016-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39971356 2020-05-15 03:37 /tmp/weather.parquet/part-00017-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40604360 2020-05-15 03:37 /tmp/weather.parquet/part-00018-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40758812 2020-05-15 03:37 /tmp/weather.parquet/part-00019-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40382015 2020-05-15 03:37 /tmp/weather.parquet/part-00020-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40181667 2020-05-15 03:37 /tmp/weather.parquet/part-00021-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40236103 2020-05-15 03:37 /tmp/weather.parquet/part-00022-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40395446 2020-05-15 03:37 /tmp/weather.parquet/part-00023-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39821857 2020-05-15 03:37 /tmp/weather.parquet/part-00024-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40418247 2020-05-15 03:37 /tmp/weather.parquet/part-00025-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40050460 2020-05-15 03:37 /tmp/weather.parquet/part-00026-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40298252 2020-05-15 03:37 /tmp/weather.parquet/part-00027-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40702652 2020-05-15 03:37 /tmp/weather.parquet/part-00028-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40528929 2020-05-15 03:37 /tmp/weather.parquet/part-00029-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40157633 2020-05-15 03:37 /tmp/weather.parquet/part-00030-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40437074 2020-05-15 03:37 /tmp/weather.parquet/part-00031-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40382705 2020-05-15 03:37 /tmp/weather.parquet/part-00032-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39730289 2020-05-15 03:37 /tmp/weather.parquet/part-00033-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40013953 2020-05-15 03:37 /tmp/weather.parquet/part-00034-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39831970 2020-05-15 03:38 /tmp/weather.parquet/part-00035-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40882114 2020-05-15 03:38 /tmp/weather.parquet/part-00036-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39763515 2020-05-15 03:38 /tmp/weather.parquet/part-00037-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40065826 2020-05-15 03:38 /tmp/weather.parquet/part-00038-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40247715 2020-05-15 03:38 /tmp/weather.parquet/part-00039-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40742694 2020-05-15 03:38 /tmp/weather.parquet/part-00040-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40173691 2020-05-15 03:38 /tmp/weather.parquet/part-00041-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40461662 2020-05-15 03:38 /tmp/weather.parquet/part-00042-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40953050 2020-05-15 03:38 /tmp/weather.parquet/part-00043-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40845591 2020-05-15 03:38 /tmp/weather.parquet/part-00044-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40107437 2020-05-15 03:38 /tmp/weather.parquet/part-00045-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39649676 2020-05-15 03:38 /tmp/weather.parquet/part-00046-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39912209 2020-05-15 03:38 /tmp/weather.parquet/part-00047-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   41212447 2020-05-15 03:38 /tmp/weather.parquet/part-00048-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   39871083 2020-05-15 03:38 /tmp/weather.parquet/part-00049-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40465828 2020-05-15 03:38 /tmp/weather.parquet/part-00050-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40511437 2020-05-15 03:38 /tmp/weather.parquet/part-00051-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40578221 2020-05-15 03:38 /tmp/weather.parquet/part-00052-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40458876 2020-05-15 03:38 /tmp/weather.parquet/part-00053-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40108325 2020-05-15 03:38 /tmp/weather.parquet/part-00054-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40518066 2020-05-15 03:38 /tmp/weather.parquet/part-00055-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop   40574923 2020-05-15 03:38 /tmp/weather.parquet/part-00056-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop    5605101 2020-05-15 03:38 /tmp/weather.parquet/part-00057-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3helper.s3_to_hdfs(\"weather.parquet\", \"/tmp/weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/05/15 03:38:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_SUCCESS' for reading\n",
      "20/05/15 03:38:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_common_metadata' for reading\n",
      "20/05/15 03:38:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_metadata' for reading\n",
      "20/05/15 03:38:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/part-r-00000-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet' for reading\n",
      "20/05/15 03:38:37 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/part-r-00001-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet' for reading\n",
      "\n",
      "Found 5 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2020-05-15 03:38 /tmp/stations.parquet/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop        894 2020-05-15 03:38 /tmp/stations.parquet/_common_metadata\n",
      "-rw-r--r--   1 hadoop hadoop       3107 2020-05-15 03:38 /tmp/stations.parquet/_metadata\n",
      "-rw-r--r--   1 hadoop hadoop     777171 2020-05-15 03:38 /tmp/stations.parquet/part-r-00000-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     860566 2020-05-15 03:38 /tmp/stations.parquet/part-r-00001-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet\n",
      "\n",
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 9.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s3helper.s3_to_hdfs(\"info/stations.parquet\", \"/tmp/stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwx   - mapred mapred          0 2020-05-15 03:33 /tmp/hadoop-yarn\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-05-15 03:38 /tmp/stations.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-05-15 03:38 /tmp/weather.parquet\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_hdfs(\"/tmp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up to here needs to be executed once each time a spark cluster is created.\n",
    "The HDFS files will stay there as long as you keep the cluster on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here down needs to be executed each time a notebooks is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 36 ms, total: 192 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = sqlContext.sql(\"SELECT * FROM parquet.`/tmp/weather.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stations=sqlContext.sql(\"SELECT * FROM parquet.`/tmp/stations.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Counting the number of stations in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_names=stations.select('ID').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(st_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_names1=[r.ID for r in st_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_names1[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C=Counter([x[:3] for x in st_names1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_prefix=sorted(C.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_prefix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = np.array([x[1] for x in sorted_prefix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul=np.cumsum(counts)\n",
    "plot(cumul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85284"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumul[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cumul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "1. Count the total number of yearXstationXmeasurement  Records (YSMRs) for each country.\n",
    "2. Count the number of YSMR's for each measurement.\n",
    "3. Compute the mean P/M std of TOBS for each country and each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
